#!/usr/bin/env bash
################################################################################
#  Licensed to the Apache Software Foundation (ASF) under one
#  or more contributor license agreements.  See the NOTICE file
#  distributed with this work for additional information
#  regarding copyright ownership.  The ASF licenses this file
#  to you under the Apache License, Version 2.0 (the
#  "License"); you may not use this file except in compliance
#  with the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
# limitations under the License.
################################################################################

#export HADOOP_CLASSPATH=/opt/cloudera/parcels/CDH/lib/hadoop-yarn/*.jar

source /etc/profile
#获取程序包hdfs remoteRunFile地址 start
#remoteRunFile=`echo -e $@ | sed  -n 's/.*hdfs:\/\/\(.*\)/\1/p'| awk '{print $1}'`
param=$@
remoteRunFile=`echo -e $@ | grep -Po "(hdfs://.*\.(jar|py))"   | awk '{print $NF}'`

if [ ! -z "$remoteRunFile" ]; then
        #获取程序包文件名
        runFile=`echo $remoteRunFile | awk -F "/" '{print $NF}'`

        #缓存基础目录
        path="/tmp/hdfs_pkg/`date '+%Y%m%d%H'`"
        mkdir -p $path
        chmod -R 777 $path

        #拼接本地文件路径
        localRunFile="$path/`date '+%M%S'`-${runFile}"

        echo "开始下载 $remoteRunFile ..."
        rm -rf $localRunFile

        hadoop fs -get $remoteRunFile $localRunFile
        if [ -f $localRunFile ];  then
                echo "下载完毕,缓存位置: $localRunFile"
        else
                echo "下载失败,请检查jar包路径是否正确"
                exit
        fi

        #将提交参数里的程序文件路径 替换成本地程序文件
        param=${param/$remoteRunFile/$localRunFile}
fi

#获取程序包hdfs remoteRunFile地址 end


target="$0"
# For the case, the executable has been directly symlinked, figure out
# the correct bin path by following its symlink up to an upper bound.
# Note: we can't use the readlink utility here if we want to be POSIX
# compatible.
iteration=0
while [ -L "$target" ]; do
    if [ "$iteration" -gt 100 ]; then
        echo "Cannot resolve path: You have a cyclic symlink in $target."
        break
    fi
    ls=`ls -ld -- "$target"`
    target=`expr "$ls" : '.* -> \(.*\)$'`
    iteration=$((iteration + 1))
done

# Convert relative path to absolute path
bin=`dirname "$target"`

# get flink config
. "$bin"/config.sh

if [ "$FLINK_IDENT_STRING" = "" ]; then
        FLINK_IDENT_STRING="$USER"
fi

CC_CLASSPATH=`constructFlinkClassPath`

log=$FLINK_LOG_DIR/flink-$FLINK_IDENT_STRING-client-$HOSTNAME.log
log_setting=(-Dlog.file="$log" -Dlog4j.configuration=file:"$FLINK_CONF_DIR"/log4j-cli.properties -Dlogback.configurationFile=file:"$FLINK_CONF_DIR"/logback.xml)
# Add HADOOP_CLASSPATH to allow the usage of Hadoop file system
exec $JAVA_RUN $JVM_ARGS "${log_setting[@]}" -classpath "`manglePathList "$CC_CLASSPATH:$INTERNAL_HADOOP_CLASSPATHS"`" org.apache.flink.client.cli.CliFrontend $param
